1. Bulk Classification Strategy (/classify/bulk)
Optimal solution for handling large payloads:
a. In order to fetch the txns from DB, it's better to get all the txns in one go instead of fetching them one by one. This can be done using SQLAlchemy's 'in_' method to filter txns by a list of IDs.
b. Further processing can be done in parallel using Python's concurrent.futures. This allows us to utilize multiple CPU cores for CPU-bound tasks like regex matching and NLP processing.
c. If client needs a sync response, we can process the txns in batches to avoid memory overload. For example, if we have 10,000 txns, we can process them in batches of 1,000.
d. If client can handle async processing, we can enqueue the job (e.g. Redis) and return a job ID for later retrieval. This prevents request timeouts and allows for more complex processing.

2. Concurrency Model
a. asyncio.semaphore to limit concurrent requests, so that event loop isn’t overwhelmed.
b. CPU-bound classification (regex, NLP, MCC matching) → offload to multiprocessing or a threadpool (concurrent.futures) so it doesn’t block event loop.
c. Async workers for DB/API calls.
d. Uvicorn workers: Scale horizontally via --workers=N (N = cores).

3. Backpressure & Timeouts
a. Request limits: Cap bulk payload size (e.g., max 20MB / 20k transactions). Reject oversized requests early in case of sync
b. Batch size cap: e.g., max_batch=1000. Larger → auto-chunk.
c. Apply request timeout (e.g., 30s for sync) and circuit breakers (via Tenacity or proxy layer).

4. Parallel I/O (DB Layer)
a. Connection pooling: SQLAlchemy connection pool tuned (e.g., 10–20 per worker).
b. Avoid N+1: Pre-load merchant/MCC mappings in memory once per worker, not per txn.

5. Caching:
a. Store the classification responses in a dict for O(1) access, frequently used category rules pre-compiled.
b. Merchant + MCC lookup tables cached in Redis or in-process dict.

6. Profiling & Complexity
a. Hot spots: Regex parsing (O(n * m) where n=txns, m=rules).
Optimizations:
a. Pre-compile regexes once at startup.
b. Use trie/dict lookups instead of repeated scans. Store the classification responses in a dict for O(1) access.
c. Profiling tools: cProfile, py-spy, line_profiler to find slow regexes/queries.
d. First targets: Regex bottlenecks, DB round trips, JSON serialization.

7. Observability
a. Metrics to expose (via Prometheus/FastAPI middleware):
b. Request throughput (/classify & /classify/bulk).
c. Latency: avg, p95, p99 per endpoint.
d. Error rates (4xx/5xx split).
e. Queue depth (if async job system used).
f. Structured logs: Include transaction_id, user_id, latency per request.
Testing under load:
a. Locust, k6 to simulate high-volume /classify/bulk and/or /classify APIs.
b. Ramp tests (linear load increase) + stress tests (burst traffic).
c. Track p95 latency + error rates to detect saturation.

Sync Batch vs Async Streaming TradeOffs

|     Aspect            |   Sync Batch (e.g., POST /classify/bulk with 10k txns)   |   Async Streaming   (e.g., POST /classify/stream with 100k+ txns)           |
| --------------------- | -------------------------------------------------------- | --------------------------------------------------------------------------- |
|   Latency (per txn)   | Higher (all txns processed before response)              | Lower (results start flowing as soon as batches are processed)              |
|   Throughput          | Good for medium payloads (≤10k)                          | Excellent for very large payloads (100k+), avoids memory blowup             |
|   Memory usage        | Entire batch must fit in memory                          | Bounded (only current batch kept in memory at a time)                       |
|   Client Experience   | One-shot response, simple client code                    | Incremental results, client must consume stream (JSONL, NDJSON, websockets) |
|   Failure Handling    | Whole batch may fail or partially succeed (need retries) | Failures isolated to specific chunks; stream can continue                   |
|   Backpressure        | Harder to enforce; large requests may overwhelm server   | Natural backpressure — results drip-feed, server controls flow              |
|   Best Use Case       | ≤10k transactions, sync API consumers (web/mobile apps)  | 100k+ transactions, integrations, batch jobs, pipelines                     |
